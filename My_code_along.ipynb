{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkP6Vba9qix2XF1ht0Rkxq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xkaycc/SALES-PREDICTION/blob/main/My_code_along.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "yhmyY5oPrfDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHJkW7TgxC0w"
      },
      "outputs": [],
      "source": [
        "# Assign the target column to y\n",
        "y = df['charges']\n",
        "# Assign the features to X (In this case we include all columns except the target column)\n",
        "X = df.drop(columns = 'charges')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wrjZnB9ax0xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking the type of object"
      ],
      "metadata": {
        "id": "pnXt2-sJx91f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "Z7iar0Cf0YGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Test Split (Model Validation)"
      ],
      "metadata": {
        "id": "2Wm8YGMr0x9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Your Path')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Yvouzdvq05rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm numbers of rows and column in a dataframe"
      ],
      "metadata": {
        "id": "pKis8GQH1S04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "LFkmqyHS1o6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this data set,determine the target you are trying to predict."
      ],
      "metadata": {
        "id": "VSM1Z-GL1w0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The target we are trying to predict\n",
        "y = df['MedHouseVal']\n",
        "# The features we will use to make the prediction\n",
        "X = df.drop(columns = 'MedHouseVal')"
      ],
      "metadata": {
        "id": "pfrnAt_e2nAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have loaded in your data frame and split it into the target (y) and features (X), you can split it into a training set and a test set."
      ],
      "metadata": {
        "id": "m9MskrV824UJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify the split, you can check the length of each:"
      ],
      "metadata": {
        "id": "Z2oE33XO7y9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the TTS from sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Train test split \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "lTqHx2mz6-YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "4BwAzU3p71kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_test)"
      ],
      "metadata": {
        "id": "54cwdAY58qgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check for y"
      ],
      "metadata": {
        "id": "yBZdjVdi81z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_train)"
      ],
      "metadata": {
        "id": "JaFsU44Y85Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_test)"
      ],
      "metadata": {
        "id": "Db1L1pCL9Dbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format for ML and Train Test Split"
      ],
      "metadata": {
        "id": "hK8y-7w7kCtM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforming Features to numerical values for Machine Learning"
      ],
      "metadata": {
        "id": "80WhPt20-sjY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmRpt6PSkFHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace the Ordinal feature (Size) with Numbers"
      ],
      "metadata": {
        "id": "NFRjV4i7M57D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define dictionary to replace\n",
        "sizes = {'S': 0, 'M': 1, 'L': 2, 'XL': 3}\n",
        "# apply the dictionary to the column in the train set\n",
        "X_train['Size'] = X_train['Size'].replace(sizes)\n",
        "X_test['Size'] = X_test['Size'].replace(sizes)\n",
        "# view the dataframe to make sure it worked\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "59HnMptCFUHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization"
      ],
      "metadata": {
        "id": "PW-nslcbcmVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "XDi1LQVPcvW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Before scaling, let us explore our original data. Notice that we are only exploring the training set"
      ],
      "metadata": {
        "id": "2c4kdc2odEcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain descriptive statistics of your features\n",
        "X_train.describe().round(0)"
      ],
      "metadata": {
        "id": "qkVwl8KddMAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate and Fit the Scaler on the Training Data"
      ],
      "metadata": {
        "id": "WjTkgxIeo6t6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3QtE6eSrUOA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate scaler\n",
        "scaler = StandardScaler()\n",
        "# fit scaler on training data\n",
        "scaler.fit(X_train)"
      ],
      "metadata": {
        "id": "NjSB7n6No_UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#In order to apply the calculations made during the fit step"
      ],
      "metadata": {
        "id": "X4Gj1pIZpITE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform training data\n",
        "train_scaled = scaler.transform(X_train)\n",
        "# transform testing data\n",
        "test_scaled = scaler.transform(X_test)\n",
        "# view the first 5 rows of train_scaled\n",
        "train_scaled[:5]"
      ],
      "metadata": {
        "id": "j1RkoIgdq1jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to covert back to Numpy array to dataframe "
      ],
      "metadata": {
        "id": "oa9jCPCPTPoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform back to a dataframe\n",
        "X_train_scaled = pd.DataFrame(train_scaled, columns=X_train.columns)\n",
        "X_train_scaled.head()"
      ],
      "metadata": {
        "id": "MXcAimQJTfiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Explore the Scaled Data"
      ],
      "metadata": {
        "id": "i0GozXdeTwuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain descriptive statistics of the scaled data\n",
        "# Use .round(2) to eliminate scientific notation and maintain 2 places after the decimal\n",
        "X_train_scaled.describe().round(2)\n"
      ],
      "metadata": {
        "id": "cAF5UiwjUWTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ordinal and OneHotEncoder  (like red,blue and green when they are not in order like low,medium and high )in Python"
      ],
      "metadata": {
        "id": "IYXpRmEsgQn2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ZstkJ6mgkAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import make_column_selector\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "9SD5sMHlJovc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info()"
      ],
      "metadata": {
        "id": "l7ZIv55pNUFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Complication_risk'].value_counts()"
      ],
      "metadata": {
        "id": "h9VoQfmjUXwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordinal\n",
        "df['Complication_risk'].replace({'Low':0, 'Med':1, 'Medium':1, 'High':2}, inplace=True)\n",
        "df['Complication_risk'].value_counts()"
      ],
      "metadata": {
        "id": "77_CST6WOuon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict \"Additional charges\" based on the other features in the dataset."
      ],
      "metadata": {
        "id": "fyN_-uAUTq4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns = ['Unnamed: 0', 'Additional_charges'])\n",
        "y = df['Additional_charges']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "09YoR87HT8-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To one hot encod the catagorical features,so we need a way to split off the categorical features from the numeric ones"
      ],
      "metadata": {
        "id": "jObZnyNLWO67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make categorical selector\n",
        "cat_selector = make_column_selector(dtype_include='object')"
      ],
      "metadata": {
        "id": "qmwUXXj3XCw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_selector(X_train)"
      ],
      "metadata": {
        "id": "Ir0Hgid2XpM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a subset of data for only categorical columns\n",
        "train_cat_data = X_train[cat_selector(X_train)]\n",
        "test_cat_data = X_test[cat_selector(X_test)]\n",
        "train_cat_data"
      ],
      "metadata": {
        "id": "KIrRs-NPbJ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')"
      ],
      "metadata": {
        "id": "KnhkPNhe2OoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate one hot encoder\n",
        "ohe_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "#fit the OneHotEncoder on the training data\n",
        "ohe_encoder.fit(train_cat_data)\n",
        "#transform both the training and the testing data\n",
        "train_ohe = ohe_encoder.transform(train_cat_data)\n",
        "test_ohe = ohe_encoder.transform(test_cat_data)\n",
        "train_ohe"
      ],
      "metadata": {
        "id": "iATuxN_t8kzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TKLx46CXQaja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to dataframe, extract new column names from encoder\n",
        "#set prefixes to original column names\n",
        "ohe_column_names = ohe_encoder.get_feature_names_out(train_cat_data.columns)\n",
        "train_ohe = pd.DataFrame(train_ohe, columns=ohe_column_names)\n",
        "test_ohe = pd.DataFrame(test_ohe, columns=ohe_column_names)\n",
        "train_ohe\n",
        "\n"
      ],
      "metadata": {
        "id": "7t3N35GSpdua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas will attempt to use the index to determine how to match the rows. When we split the data, it mixes up the index, and when we one-hot encode the data, it resets the index. The rows stay in the same order, but the indices no longer match. We can fix that by resetting the index of the numeric columns. This is an important step."
      ],
      "metadata": {
        "id": "dV_5ksB_3ZtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a numeric selector\n",
        "num_selector = make_column_selector(dtype_include='number')\n",
        "# isolate the numeric columns\n",
        "train_nums = X_train[num_selector(X_train)].reset_index(drop=True)\n",
        "test_nums = X_test[num_selector(X_test)].reset_index(drop=True)\n",
        "# re-combine the train and test sets on axis 1 (columns)\n",
        "X_train_processed = pd.concat([train_nums, train_ohe], axis=1)\n",
        "X_test_processed = pd.concat([test_nums, test_ohe], axis=1)\n",
        "X_train_processed"
      ],
      "metadata": {
        "id": "WKh8lYTo4EY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIMPLEIMPUTER FOR CLEANING"
      ],
      "metadata": {
        "id": "G3sjBPDP_vOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ],
      "metadata": {
        "id": "sM-FdTJC3xrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isna().sum().sum(), 'missing values')"
      ],
      "metadata": {
        "id": "EXUwagdtvK5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In a real project we can investigate further with this code \n",
        "#The code below filters the dataset for just the rows that are missing at least 1 value and shows the shape. \n",
        "#We could drop rows or columns before the validation split without leaking data\n",
        "df[df.isna().any(axis=1)].shape"
      ],
      "metadata": {
        "id": "1G5d_rod2iI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note before this you must have divided your X and y variable and done your train test split"
      ],
      "metadata": {
        "id": "5FHgmYFk4Qec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_selector = make_column_selector(dtype_include='number')\n",
        "cat_selector = make_column_selector(dtype_include='object')\n",
        "#select the numeric columns of each type\n",
        "num_columns = num_selector(X_train)\n",
        "cat_columns = cat_selector(X_train)\n",
        "#check our lists\n",
        "print('numeric columns are', num_columns)\n",
        "print('categorical columns are', cat_columns)"
      ],
      "metadata": {
        "id": "d8OqfZtD4FdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EujZLJP62fow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we decide which strategy to use for imputation, we need to understand our data. The code below will isolate the numeric columns that are missing data. We can do this to see "
      ],
      "metadata": {
        "id": "YwflF0u85Tg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# isolate the numeric columns\n",
        "df_num = df[num_columns]\n",
        "# isolate the columns with missing data\n",
        "df_num.loc[:, df_num.isna().any()]"
      ],
      "metadata": {
        "id": "u9HjA-6V49iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple Imputer without ColumnTransformer\n",
        "\n",
        "First, let's check which columns are missing data."
      ],
      "metadata": {
        "id": "D-PoWxDL7jL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.isna().any()"
      ],
      "metadata": {
        "id": "TV2KHmzV7lVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the imputer object from the SimpleImputer class with strategy 'median'\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "#Fit the imputer object on the numeric training data with .fit() \n",
        "#calculates the medians of the columns in the training set\n",
        "median_imputer.fit(X_train[num_columns])\n",
        "#Use the median from the training data to fill the missing values in \n",
        "#the numeric columns of both the training and testing sets with .transform()\n",
        "X_train.loc[:, num_columns] = median_imputer.transform(X_train[num_columns])\n",
        "X_test.loc[:, num_columns] = median_imputer.transform(X_test[num_columns])"
      ],
      "metadata": {
        "id": "fonWft8E8lTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for both numeric and catagorical value.we can use a ColumnTransformer to impute missing values in both numeric and categorical columns at the same time. After instantiate column selectors for object and numeric values,insatntiate **IMputers**"
      ],
      "metadata": {
        "id": "WArstAYmvmHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate the selectors to for numeric and categorical data types\n",
        "num_selector = make_column_selector(dtype_include='number')\n",
        "cat_selector = make_column_selector(dtype_include='object')"
      ],
      "metadata": {
        "id": "Eb-Xo9O6yRsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate SimpleImputers with most_frequent and median strategies\n",
        "freq_imputer = SimpleImputer(strategy='most_frequent')\n",
        "median_imputer = SimpleImputer(strategy='median')"
      ],
      "metadata": {
        "id": "cLS0Q8Ud5WO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the ColumnTransformer"
      ],
      "metadata": {
        "id": "uwLDDb0Ez-rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create tuples of (imputer, selector) for each datatype\n",
        "num_tuple = (median_imputer, num_selector)\n",
        "cat_tuple = (freq_imputer, cat_selector)\n",
        "# instantiate ColumnTransformer\n",
        "col_transformer = make_column_transformer(num_tuple, cat_tuple, remainder='passthrough')\n",
        "col_transformer\n"
      ],
      "metadata": {
        "id": "hRnF076iwIz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impute Missing Values With ColumnTransformer"
      ],
      "metadata": {
        "id": "wBR5HUzs2v1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit ColumnTransformer on the training data\n",
        "col_transformer.fit(X_train)\n",
        "# transform both the training and testing data (this will output a NumPy array)\n",
        "X_train_imputed = col_transformer.transform(X_train)\n",
        "X_test_imputed = col_transformer.transform(X_test)\n",
        "# change the result back to a dataframe\n",
        "X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
        "X_train_imputed.isna().any()"
      ],
      "metadata": {
        "id": "2xKFCvAz2tUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pipeline contains multiple transformers (or even models!) and performs operations on data IN SEQUENCE. Compare this to ColumnTransformers that perform operations on data IN PARALLEL"
      ],
      "metadata": {
        "id": "l71-0Wvt48Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pipeline"
      ],
      "metadata": {
        "id": "Ago_O_k6-qse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ],
      "metadata": {
        "id": "0A8h6vNv5CHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "steps in pipeline: import necessary library,load data,inspect data , Validation Split(target,feature,X_train X_test),instantiate the transformer and instanstiate the pipeline"
      ],
      "metadata": {
        "id": "hF0WrZoUCOtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#inspect the data\n",
        "print(df.info(), '\\n')\n",
        "print(df.isna().sum())\n"
      ],
      "metadata": {
        "id": "A_Q2iY7b-yfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we COULD use ColumnTransformer to split the columns by integers and floats and apply mean imputation to the floats and median imputation to the integers, and then scale them all"
      ],
      "metadata": {
        "id": "aoW0r6gNEayO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# divide features and target and perform a train/test split.\n",
        "X = df.drop(columns=['Life expectancy'])\n",
        "y = df['Life expectancy']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
      ],
      "metadata": {
        "id": "9dfXzTSKEkr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "instantiate transformers"
      ],
      "metadata": {
        "id": "uQux0qHKE4Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate an imputer and a scaler\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "2jU9CkDUFDFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "instantiate pipeline"
      ],
      "metadata": {
        "id": "ZoqPOPHBGFg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#combine the imputer and the scaler into a pipeline\n",
        "preprocessing_pipeline = make_pipeline(median_imputer, scaler)\n",
        "preprocessing_pipeline"
      ],
      "metadata": {
        "id": "RIdh77xtEvS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fit the pipeline on a train data"
      ],
      "metadata": {
        "id": "PgLpie4fGQbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fit the pipeline on a train data "
      ],
      "metadata": {
        "id": "lphJTdFQGLwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_pipeline.fit(X_train)"
      ],
      "metadata": {
        "id": "U2WXhw1eGmZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform both the training data and the testing data"
      ],
      "metadata": {
        "id": "gcZHdBCpG5Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transform train and test sets\n",
        "X_train_processed = preprocessing_pipeline.transform(X_train)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "#inspect the result of the transformation\n",
        "print(np.isnan(X_train_processed).sum().sum(), 'missing values \\n')\n",
        "X_train_processed\n",
        "\n",
        "#Scikit-Learn transformers and pipelines always return Numpy arrays, not Pandas dataframes"
      ],
      "metadata": {
        "id": "9yKoTGk_G7IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary : ColumnTransformer is used for applying different preprocessing steps to different subsets of features, while Pipeline is used to chain together multiple steps in a machine learning workflow, including data preprocessing and modeling"
      ],
      "metadata": {
        "id": "poG6oSZYcHMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pipeline and column transformer together\n",
        "steps after validating split - ctpc-then transform the data"
      ],
      "metadata": {
        "id": "1v6V6q12en1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ordinal encoding\n",
        "# Ordinal Encoding 'Complication_risk'\n",
        "replacement_dictionary = {'High':2, 'Medium':1, 'Med':1, 'Low':0}\n",
        "df['Complication_risk'].replace(replacement_dictionary, inplace=True)\n",
        "df['Complication_risk']"
      ],
      "metadata": {
        "id": "eEBiPLRvfrIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the next step is validation split(step 4) where you define features"
      ],
      "metadata": {
        "id": "3IGep_HljQdB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 5  I nstantiate column selector step 6 Instantiate column transformers"
      ],
      "metadata": {
        "id": "fXrCGVXcj3eF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6\n",
        "# Imputers\n",
        "freq_imputer = SimpleImputer(strategy='most_frequent')\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "# Scaler\n",
        "scaler = StandardScaler()\n",
        "# One-hot encoder\n",
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)"
      ],
      "metadata": {
        "id": "kzNh-mYwkHJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 7 instantiate pipeline\n",
        "# Numeric pipeline\n",
        "numeric_pipe = make_pipeline(mean_imputer, scaler)\n",
        "numeric_pipe"
      ],
      "metadata": {
        "id": "gHaZ6E3Yi4GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical pipeline\n",
        "categorical_pipe = make_pipeline(freq_imputer, ohe)\n",
        "categorical_pipe"
      ],
      "metadata": {
        "id": "Qzv6brpel-9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 8 instantiate column transformer\n",
        "# Tuples for Column Transformer\n",
        "number_tuple = (numeric_pipe, num_selector)\n",
        "category_tuple = (categorical_pipe, cat_selector)\n",
        "# ColumnTransformer\n",
        "preprocessor = make_column_transformer(number_tuple, category_tuple)\n",
        "preprocessor\n"
      ],
      "metadata": {
        "id": "4OJ8F2ktmlwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 9 transformer data\n",
        "# fit on train\n",
        "preprocessor.fit(X_train)"
      ],
      "metadata": {
        "id": "MInZHQHhoUsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fit method worked to fit all 4 transformers inside the ColumnTransformer. We will use that fitted ColumnTransformer to transform both our training and testing datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "ss_PPlEBo4ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform train and test\n",
        "X_train_processed = preprocessor.transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n"
      ],
      "metadata": {
        "id": "qbwJ0Lt1o59f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "step 10 Inspect result"
      ],
      "metadata": {
        "id": "kgjmQru-pUjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " Check for missing values and that data is scaled and one-hot encoded\n",
        "print(np.isnan(X_train_processed).sum().sum(), 'missing values in training data')\n",
        "print(np.isnan(X_test_processed).sum().sum(), 'missing values in testing data')\n",
        "print('\\n')\n",
        "print('All data in X_train_processed are', X_train_processed.dtype)\n",
        "print('All data in X_test_processed are', X_test_processed.dtype)\n",
        "print('\\n')\n",
        "print('shape of data is', X_train_processed.shape)\n",
        "print('\\n')\n",
        "X_train_processed"
      ],
      "metadata": {
        "id": "YxJnqw13pZy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# classification\n"
      ],
      "metadata": {
        "id": "Jn8gsXqu7z2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MatplotLib.pyplot as plt\n",
        "import Pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "xMLno0eMoqq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ". Find the Class Names and Determine How Balanced the Classes Are."
      ],
      "metadata": {
        "id": "gTLsiqdnFT2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# by using normalize = True with value_counts, our output is the percentage of each class (written as a decimal)\n",
        "df['Species'].value_counts(normalize = True)"
      ],
      "metadata": {
        "id": "GT-vlpSWE4e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the String Names of the Classes to Numeric Values"
      ],
      "metadata": {
        "id": "_8BGDCxSGNzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Species'] = df['Species'].replace({'Iris-setosa': 0, 'Iris-virginica': 1, 'Iris-versicolor': 2})"
      ],
      "metadata": {
        "id": "xoFFU2Bv78GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Arrange Data into a Features Matrix and Target Vector"
      ],
      "metadata": {
        "id": "Xk7-l5K4G47V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Species']\n",
        "X = df.drop(columns = 'Species')"
      ],
      "metadata": {
        "id": "l-G7_p_kHD7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Train Test Split (model validation)."
      ],
      "metadata": {
        "id": "fDGDOHhWHMAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "MJ5gjtiHHiHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier "
      ],
      "metadata": {
        "id": "HW7CBMS0HnsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make an instant model\n",
        "Dec_tree = DecisionTreeClassifier(randon_state =42,max_depth=2)"
      ],
      "metadata": {
        "id": "aT6cA2efHzMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at some hyperparameters that seem tunable\n",
        "dec_tree.get_params()"
      ],
      "metadata": {
        "id": "x8iilLgSJteH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model on the Data, Storing the Information Learned From the Data. The model is learning the relationship between x (features-sepal width, sepal height, etc) and y (labels-species of iris)."
      ],
      "metadata": {
        "id": "s6Q1gbOlKlvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dec_tree.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "MSr72TzNLUc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the Labels of New Data (new flowers)\n",
        "dec_tree.predict(X_test)"
      ],
      "metadata": {
        "id": "j0T9oLq-MWXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Your Model Performance"
      ],
      "metadata": {
        "id": "B0527broM4EL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate classification accuracy\n",
        "train_score = dec_tree.score(X_train, y_train)\n",
        "test_score = dec_tree.score(X_test, y_test)\n",
        "print(train_score)\n",
        "print(test_score)"
      ],
      "metadata": {
        "id": "KNwxCysQM6Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LINEAR REGRESSION\n",
        "\n",
        "Note to avoid repetion,we assume we have done 'assigned variable' and train_test split.\n",
        "**Preprocessing**\n",
        "\n",
        "You've learned many ways to preprocess data. In this case, all columns are numeric and there is no missing data. The only preprocessing we need to do is scaling.\n"
      ],
      "metadata": {
        "id": "8wEuF3HTlI-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#step 0: from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "7o2x_yZYEtOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Import the and instantiate the model.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression()"
      ],
      "metadata": {
        "id": "zMMak3wLCaX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Create a modeling pipeline.\n",
        "from sklearn.pipeline import make_pipeline\n",
        "reg_pipe = make_pipeline(scaler, reg)"
      ],
      "metadata": {
        "id": "kIXGppByDF9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Train the model on your training data(This is the step where the model \"learns\" about the relationship between the features and target. \n",
        "\n",
        "#Model is learning the relationship between X and y)\n",
        "reg_pipe.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "tKwg-FtHGxyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now that the model has \"learned\" the patterns, it is time to see how well it is able to make predictions.\n",
        "#Step 4: Make predictions using the testing data\n",
        "predictions = reg_pipe.predict(X_test)\n"
      ],
      "metadata": {
        "id": "uLPnO3WvILty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bonus(not required but for future)\n",
        "prediction_df = X_test.copy()\n",
        "prediction_df['True Median Price'] = y_test\n",
        "prediction_df['Predicted Median Price'] = predictions\n",
        "prediction_df['Error'] = predictions - y_test\n",
        "prediction_df.head()"
      ],
      "metadata": {
        "id": "hLuYQbeaLJZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Pipelines in Python (regression)\n",
        "IL epccf"
      ],
      "metadata": {
        "id": "AtrA89g9yUaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 import data\n",
        "import pandas as pd\n",
        "from sklearn.compose import make_column_transformer, make_column_selector\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn import set_config\n",
        "set_config(display='diagram')"
      ],
      "metadata": {
        "id": "vZ0x-h3lyeyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2 Load data\n",
        "# Import the data\n",
        "path = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vSzb_CfjmApDMSXRn-Ga8X5rgoRVm7U_UNYotqQ0iW2JVx1qoKFr41XOA-FNKPqds83B0oUM6zKtLqK/pub?output=csv'\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "y8JM76K13S0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore data\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "z_po19Z09D1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvHYk20vyT0l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}